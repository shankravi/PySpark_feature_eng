{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS/CMPSC 410 MiniProject #1\n",
    "\n",
    "Learning Objectives\n",
    "- Be able to transform columns of Big Data for feature engineering using DataFrame.\n",
    "- Be able to perform simple data analytics using SQL Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "from pyspark.sql.functions import col, column\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, IndexToString\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SparkSession.builder.master(\"local\").appName(\"DataFrameSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scanners_df = ss.read.csv(\"/storage/home/juy1/work/Darknet/scanners-dataset1-anon.csv\", header= True, inferSchema=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can use printSchema() to display the schema of the DataFrame Scanners_df to see whether it was inferred correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scanners_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A Transfosrm the feature \"host_services_per_censys\" into an array of services.\n",
    "#### The original value of the column is a string that connects all the network services/ports open by a sourceIP/scanner.\n",
    "#### The different services/ports open are connected by dash \"-\". For example, \"81-161-2000\" indicates the\n",
    "#### scanner has three ports/services open: port 81, port 161, and port 2000.\n",
    "#### Therefore, we want to use split to separate it into an array of ports/services open by each scanner.\n",
    "#### This transformation is important because it enables features such as the following to be created:\n",
    "#### (1) Whether the ports/services used by the scanner include those used by a known malware.\n",
    "#### (2) Whether the ports/services used by a scanner overlaps with those used by another scanner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The original value of the column \"host_services_per_censys\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scanners_df.select(\"host_services_per_censys\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We want to find the services/ports that most scanners have them open. There are multiple ways to find this information using Spark.  We will demonstrate one below, which involves the following steps:\n",
    "1. Convert the DataFrame Scanners_df into an RDD\n",
    "2. Map to each row of the DF-converted RDD to extract the column \"host_services_per_censys\". Save the mapping result in a new RDD (which contains only values of the column).\n",
    "3. Use flatMap to split to the string (using \"-\" as the delimiter) to convert the RDD into an RDD of ports/services that are open on the host of the scanner.\n",
    "4. Use map to generate a key-value pair RDD, where key is a port/service opens on a scanner, the value is 1.\n",
    "5. Use reduceByKey to count the total number of scanners that have a specific port/service open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert the DataFrame Scanners_df into an RDD\n",
    "Scanners_RDD = Scanners_df.rdd\n",
    "Scanners_RDD.persist().take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Map to each row of the DF-converted RDD to extract the column \"host_services_per_censys\". Save the mapping result \n",
    "# in a new RDD (whih contains only values of the column)\n",
    "Host_services_column = Scanners_RDD.map(lambda row: row.host_services_per_censys)\n",
    "Host_services_column.persist().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can transform the string into a list of services (i.e., port numbers) that the scanner has opened using map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Host_services_rdd=Host_services_column.map(lambda string: string.split(\"-\"))\n",
    "Host_services_rdd.persist().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: However, in order to count how many scanners are keeping a specific port/service open, it is easier to use flatMap (instead of map above) to \"flatten\" the results of splitting (using \"-\" as the delimiter to convert the input RDD into an RDD of all ports/services that are open on the host of each sourceIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Host_services_f_list = Host_services_column.flatMap(lambda string: string.split(\"-\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Use map to generate a key-value pair RDD, where key is a port/service opens on a scanner, the value is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Host_services_count = Host_services_f_list.map(lambda s: (s, 1) ) \n",
    "Host_services_count.persist().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Use reduceByKey to count the total number of scanners that have a specific port/service open.\n",
    "## Exercise 1 (7 points): Complete the following two cells to do the following remaining steps\n",
    "1. Calculate the total number of scanners that have each port/service open. \n",
    "2. Sort them in descending order of count so that we can see the port/services that are open for most scanners. Save the resulted file in a directory you specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1-1\n",
    "Host_services_total= Host_services_count.reduceByKey(lambda ???)\n",
    "Host_services_total.persist().take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 1-2\n",
    "Count_Services = Host_services_total.map(lambda x: ??? ) )\n",
    "Sorted_Count_Services = Count_Services.sortByKey(???)\n",
    "Sorted_Count_Services.top(100)\n",
    "Sorted_Count_Services.saveAsTextFile(\"????\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: We want to add a column to the original DataFrame such that the new column stores the array of services/ports opened by each scanner.\n",
    "## From Spark programming viewpoint, this involves adding a column of a complex data type (e.g., Array of strings).\n",
    "### This can be done by \"selecting\" from a DataFrame the result of applying split to a string using a delimiter (e.g., \"-\" in our case)\n",
    "### Notice 1: This split applies to each row of the DataFrame.  Therefore, it needs to use \"col\" of Spark SQL to refer to a specific column of the row.\n",
    "### Notice 2: The result of this is a new DataFrame with only one column, which is the result of select(split...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scanners_df.select(split(col(\"host_services_per_censys\"), \"-\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Often, we are especially interested in scanners who have a specific port open because it may indicate that the scanner is a specific type of malware.  An example of this is port 23 used by the Mirai botnet.\n",
    "## Whether a scanner has a specific port (e.g., port 23) open can be determined based on whether the array generated above by splitting \"host_services_per_censys\" contains the port (e.g., port 23)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "Scanners_df.select(array_contains(split(col(\"host_services_per_censys\"), \"-\"), \"23\")).show(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Because the array of services is easier to process (than the original string), it is desirable to save it as an extra column in the original DataFrame (so that we do not need to spend time doing the \"split\" in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can use DataFrame.withColumn to add a column to a Data Frame.  The code below attempts to add the two new columns we generated above:\n",
    "- A column that contains the array of all hosts/ports opened by a scanner.\n",
    "- A column that contains a Boolean value indicating whether the array of services contain a specific port (e.g., port 23)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 (8 points) Did the list of columns include the previous column we added (i.e., \"Services_Array\")?\n",
    "1. Check whether the last DF contains the two new columns?  Explain the result.\n",
    "2. Fix the problem by typing your modified code in the code cell \"Exercise 2 Solutions:\". You should evaluate the cells below (makred as #Exercise 2 code). However, DO NOT MODIFY THE code in these cells. \n",
    "3. Explain why the solution works in the MarkDown Cell of \"Answer to Exercise 2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 (DO NOT CHANGE)\n",
    "Scanners_df.withColumn(\"Services_Array\", split(col(\"host_services_per_censys\"), \"-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 (DO NOT CHANGE)\n",
    "Scanners_df.withColumn(\"Services_Array\", split(col(\"host_services_per_censys\"), \"-\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 (do not change)\n",
    "Scanners_df.withColumn(\"Service_P23\", array_contains(split(col(\"host_services_per_censys\"), \"-\"), \"23\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 (do not change)\n",
    "Scanners_df.withColumn(\"Service_P23\", array_contains(split(col(\"host_services_per_censys\"), \"-\"), \"23\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 (do not change)\n",
    "Scanners_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2  Solutions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer to Exercise 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C. Create a Table so that we can post an SQL Query to the Table for simple Data Analytics\n",
    "## Exercise 3 (6 points) Below are some examples of data analytics questions.  Which of them can be answered by an SQL query? \n",
    "1. Which country has the largest number of scanners who scan at least 10 ports?\n",
    "2. Which service/port is open for the largest number of scanners (based on Censys)?\n",
    "3. How many scanners includes port 21 in the list of ports it scanned?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer to Exercise 3:\n",
    "1. Which country has the largest number of scanners who scan at least 10 ports?\n",
    "2. Which service/port is open for the largest number of scanners (based on Censys)?\n",
    "3. How many scanners includes port 21 in the list of ports it scanned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scanners_df3.createOrReplaceTempView(\"Scanners_sql_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can then submit an SQL Query to a View (i.e., temporary Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlQuery1 = ss.sql(\"SELECT * FROM Scanners_sql_view where country = 'Thailand'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlQuery1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlQuery2 = ss.sql(\"SELECT * FROM Scanners_sql_view where num_ports_scanned > 20\")\n",
    "sqlQuery2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlQuery3 = ss.sql(\"SELECT count(sourceIP) FROM Scanners_sql_view where num_ports_scanned > 10\")\n",
    "sqlQuery3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlQuery4 = ss.sql(\"SELECT country, count(sourceIP) FROM Scanners_sql_view GROUP BY country \")\n",
    "sqlQuery4.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlQuery5 = ss.sql(\"SELECT country, count(sourceIP) FROM Scanners_sql_view WHERE Service_P23=True GROUP BY country\")\n",
    "sqlQuery5.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL can also sort the query result by a specific column using ORDER BY.\n",
    "## If we want to sort them by descreasing order, we use ORDER BY ... DESC.  (The default is ascending order.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlQuery6=ss.sql(\"SELECT country, count(sourceIP) FROM Scanners_sql_view WHERE Service_P23=True GROUP BY country \\\n",
    "ORDER BY count(sourceIP) DESC\")\n",
    "sqlQuery6.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 (9 points) Write the SQL query to find the total number of scanners (by country) who scanned at least 5 ports, sort them in descending order of the count. Show the top 20 countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlQuery4 = ss.sql(\"SELECT ??? FROM ??? WHERE ??? \\\n",
    "GROUP BY ??? ORDER BY ???  ???\")\n",
    "sqlQuery4.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
